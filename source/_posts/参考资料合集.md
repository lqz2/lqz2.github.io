---
title: 参考资料合集
date: 2023-08-11 17:28:17
categories: 资料
math:
tags:
sticky: 10000
---
<!-- TOC -->

- [一些有价值的参考资料（持续更新!）:rainbow::rainbow::rainbow:](#一些有价值的参考资料持续更新rainbowrainbowrainbow)
    - [第1周:rainbow:](#第1周rainbow)
    - [第2周:rainbow:](#第2周rainbow)
    - [第3周:rainbow:](#第3周rainbow)
    - [第4周:rainbow:](#第4周rainbow)
    - [第5周:rainbow:](#第5周rainbow)
    - [第六周:rainbow:](#第六周rainbow)

<!-- /TOC -->
## 一些有价值的参考资料（持续更新!）:rainbow::rainbow::rainbow:

### 第1周:rainbow:

1.  了解了在线深度学习可视化辅助工具wandb
2.  关于结构性相似度（SSIM）的论文，《Image quality assessment: from error visibility to structural similarity》
3.  在TarDAL中使用显著性检测从红外图像中生成mask的网络，《R3Net: Recurrent Residual Refinement Network for Saliency Detection》
4.  GAN网络中为什么判别器训练的越好，生成器梯度消失越严重，参考[1](https://blog.csdn.net/qq_42693593/article/details/127365516?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169372361316800182792093%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=169372361316800182792093&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-127365516-null-null.142^v93^control&utm_term=%E4%B8%BA%E4%BB%80%E4%B9%88%E5%88%A4%E5%88%AB%E5%99%A8%E8%AE%AD%E7%BB%83%E7%9A%84%E8%B6%8A%E5%A5%BD%EF%BC%8C%E7%94%9F%E6%88%90%E5%99%A8%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E8%B6%8A%E4%B8%A5%E9%87%8D&spm=1018.2226.3001.4187)
5.  在TarDAL论文中，判别器损失函数使用了WGAN-div损失函数，即引入了Wasserstein散度，具体参考(https://zhuanlan.zhihu.com/p/25071913)，[1](https://blog.csdn.net/qq_39237205/article/details/123718856?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169372388816800182120614%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=169372388816800182120614&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-5-123718856-null-null.142^v93^control&utm_term=wgan-div&spm=1018.2226.3001.4187)，总的来说WGAN可以帮助GAN网络的训练变的更加稳定。
6.  关于图像融合评价指标的计算：[1](https://blog.csdn.net/fovever_/article/details/129332278?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169372348316800188584429%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169372348316800188584429&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-129332278-null-null.142^v93^control&utm_term=%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87python&spm=1018.2226.3001.4187)
### 第2周:rainbow:
1. linux下通过软连接切换cuda版本，首先 ls /usr/local | grep cuda查看已安装的cuda版本，然后sudo ln -snf /usr/local/cuda-10.0 /usr/local/cuda建立软连接即可，-s表示创建软连接，-n表示软连接已存在则删除，-f表示强制覆盖，具体可参考[1](https://blog.csdn.net/fxmtb/article/details/130573696?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169371218816800213030057%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=169371218816800213030057&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-130573696-null-null.142^v93^control&utm_term=linux%E4%B8%8B%E5%88%87%E6%8D%A2cuda%E7%89%88%E6%9C%AC%E8%BD%AF%E8%BF%9E%E6%8E%A5&spm=1018.2226.3001.4187)，如果没有root权限，可通过修改~/.bashrc文件的方法切换，具体参考[2](https://blog.csdn.net/Marquis_Z/article/details/129683777?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169371196516800211516194%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169371196516800211516194&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-129683777-null-null.142^v93^control&utm_term=linux%E4%B8%8B%E5%88%87%E6%8D%A2cuda%E7%89%88%E6%9C%AC&spm=1018.2226.3001.4187),[3](https://blog.csdn.net/deersonglzx/article/details/132103228?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169371204116800188517144%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=169371204116800188517144&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~times_rank-1-132103228-null-null.142^v93^control&utm_term=linux%E4%B8%8B%E5%88%87%E6%8D%A2cuda%E7%89%88%E6%9C%AC&spm=1018.2226.3001.4187)
2. vscode远程调试中文件路径无法找到时，可以将launch.json中pargram设置为运行文件，cwd设置为运行目录，这样即可正常调试。
3. 调试过程查看梯度：在vscode调试时，尝试在调试控制台查看模型的梯度，但是只能在变量中看到nn.module.parameters()函数，要具体查看参数需要通过代码将梯度打印出来，如for name, param in self.detect.net.named_parameters(): print(name, param.grad)，经过调试得出，在优化器更新参数时，只有模型的loss经过backward()回传才会计算梯度，比如gloss=a * floss+b * dloss，如果a=0，那么gloss.backward()后，会计算模型d梯度而不会计算模型f的梯度。
4. pytorch中permute函数和view()/reshape()函数的区别，前者是在维度上互换或者重新排列，后者相当于将所有元素重新取出再排列。详细参考[1](https://blog.csdn.net/Dust_Evc/article/details/128565263?ops_request_misc=&request_id=&biz_id=102&utm_term=permute%E5%92%8Cview/reshape%E5%8C%BA%E5%88%AB&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-128565263.142^v93^control&spm=1018.2226.3001.4187),[2](https://blog.csdn.net/ODIMAYA/article/details/123898732?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169371133116800226577302%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=169371133116800226577302&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-11-123898732-null-null.142^v93^control&utm_term=permute%E5%92%8Cview%2Freshape%E5%8C%BA%E5%88%AB&spm=1018.2226.3001.4187)，同时torch.nn.Flatten与torch.flatten也有点小区别，即前者默认从维度1开始，后者默认从维度0开始。
5. einops库中的矩阵操作与einsum矩阵乘法，参考：[1](https://blog.csdn.net/a486259/article/details/126966772?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169370823316800182774809%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=169370823316800182774809&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-6-126966772-null-null.142^v93^control&utm_term=einops%E4%B8%8Eeinsum&spm=1018.2226.3001.4187)
6. 关于dataloader中的collate_fn函数，他的作用是手动调整1个batch中的数据的组织方式，其传入参数为batch(1个大小为batchsize的list)，具体参考[1](https://blog.csdn.net/dong_liuqi/article/details/114521240)，如果__getitem__函数中出现标签为空的情况，可以通过collate_fn进行改动，参考[2](https://blog.csdn.net/guyuealian/article/details/91129367###)
7. pytorch中torchvision.transform.Resize()和torch.nn.functions.interpolate的区别：前者是基于插值实现的，用于调整图像的大小，后者常用于对特征图进行上采样等操作。更多关于图像预处理的方法，参考torchvision官方文档(https://pytorch.org/vision/0.15/transforms.html#transforms-scriptability)
8. IEEE Conference latex模板使用注意事项，参考[1](https://blog.csdn.net/Hsin96/article/details/121133441)，latex列表的使用，参考[2](https://blog.csdn.net/xovee/article/details/106365532?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169372332716800188537784%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169372332716800188537784&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-4-106365532-null-null.142^v93^control&utm_term=latex%E5%88%97%E8%A1%A8&spm=1018.2226.3001.4187)
9. ppt绘图后使用vba宏1键导出为pdf并裁剪，参考[1](https://blog.csdn.net/leida_wt/article/details/114326133?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169372340516800180676210%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=169372340516800180676210&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-2-114326133-null-null.142^v93^control&utm_term=ppt%E4%BD%BF%E7%94%A8vba%E5%AF%BC%E5%87%BApdf&spm=1018.2226.3001.4187)，这里vba代码中使用pdfcrop在我的电脑中失效，可以shell代码改为Shell "pdfcrop " & strNotes & sExt & " " & strNotes & sExt，然后在前面加上ChDir sPath更改工作路径，或者手动使用pdfcrop进行pdf裁剪
10. 科研作图和表格参考：(https://zhuanlan.zhihu.com/p/603088040)
11. 统计模型参数量和计算量，参考[1](https://blog.csdn.net/qq_43426908/article/details/130338449?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169370415016800182723428%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=169370415016800182723428&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~times_rank-4-130338449-null-null.142^v93^control&utm_term=torchstat&spm=1018.2226.3001.4187)
12. etc.，et al.，i.e.，e.g.的区别，参考(https://zhuanlan.zhihu.com/p/85630819)
13. 图像的相关系数计算，参考[1](https://blog.csdn.net/weixin_30394333/article/details/95754064?ops_request_misc=&request_id=&biz_id=102&utm_term=%E5%9B%BE%E5%83%8F%E7%9A%84%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E8%AE%A1%E7%AE%97&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-95754064.142^v93^control&spm=1018.2226.3001.4187)
14. 关于范数，0范数表示非零元素个数，1范数表示所有元素绝对值之和，2范数表示所有元素间平方和，无穷范数表示所有元素最大值，对于两个向量，L1范数又叫曼哈顿距离，是元素间的绝对误差和(差的绝对值求和)，而L2范数是欧氏距离，是元素间的平方差和。
15. 梯度裁剪nn.utils.clip_grad_norm_的使用，参考[1](https://blog.csdn.net/Mikeyboi/article/details/119522689)，[2](https://blog.csdn.net/zhaohongfei_358/article/details/122820992?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169365178316800213092562%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169365178316800213092562&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-122820992-null-null.142^v93^control&utm_term=nn.utils.clip_grad_norm_&spm=1018.2226.3001.4187)
16. 本地文件传输到远程服务器，远程服务器文件互相传输，参考：(https://cloud.tencent.com/developer/article/2092491)
17. 服务器防火墙设置，参考[1](https://blog.csdn.net/lu962820662/article/details/129340504?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169365169716800211535651%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=169365169716800211535651&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~times_rank-3-129340504-null-null.142^v93^control&utm_term=linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%98%B2%E7%81%AB%E5%A2%99%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8&spm=1018.2226.3001.4187)
18. 关于自注意力的意义的详细解析，参考(https://zhuanlan.zhihu.com/p/410776234)
19. BatchNorm，LayerNorm，GroupNorm的区别，参考：[1](https://blog.csdn.net/qq_43426908/article/details/123119919?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169365115016800225577832%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169365115016800225577832&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-123119919-null-null.142^v93^control&utm_term=batchnorm%20layernorm%20%20groupnorm&spm=1018.2226.3001.4187)
20. python广播机制的用法，参考：(https://zhuanlan.zhihu.com/p/86997775)
21. h5py库的用法，参考[1](https://blog.csdn.net/csdn15698845876/article/details/73278120?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%2273278120%22%2C%22source%22%3A%22qq_45909764%22%7D&fromshare=blogdetail)

### 第3周:rainbow:
1. 两台服务器之间传输文件的方法，参考[1](https://blog.csdn.net/Black_8/article/details/122925549?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22122925549%22%2C%22source%22%3A%22qq_45909764%22%7D&fromshare=blogdetail)
2. 关于python中文件路径的介绍，参考[1](https://blog.csdn.net/The_Time_Runner/article/details/84147220?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%2284147220%22%2C%22source%22%3A%22qq_45909764%22%7D&fromshare=blogdetail)
3. 关于python中super()函数，有时候看到在super函数中传递当前类的实例如super(AttentionBase, self).__init__()，又是则不传递，如super().__init__()，在Python 3中，如果super()不传递任何参数，则函数将自动使用当前类和实例作为参数。因此直接super().__init__()与前者是等效的。
4. list，numpy，tensor互相转换及常用操作，参考：[1](https://blog.csdn.net/onion_rain/article/details/107460601?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22107460601%22%2C%22source%22%3A%22qq_45909764%22%7D&fromshare=blogdetail)
5. 对list进行[:None]和[None:]的作用，参考：(https://zhuanlan.zhihu.com/p/598562546)
6. 关于使用np.percentile计算百分位数的用法，参考[1](https://blog.csdn.net/yxf771hotmail/article/details/131849484?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22131849484%22%2C%22source%22%3A%22qq_45909764%22%7D&fromshare=blogdetail)
7. python导入上级目录. 同级目录. 下级目录的包，参考[1](https://blog.csdn.net/gaifuxi9518/article/details/81038818?ops_request_misc=%7B%22request%5Fid%22%3A%22169382667516800197050835%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=169382667516800197050835&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-81038818-null-null.142^v93^control&utm_term=python%E5%AF%BC%E5%85%A5%E5%8C%85%E7%9A%84%E5%8E%9F%E7%90%86&spm=1018.2226.3001.4187)，[2](https://blog.csdn.net/anshiquanshu/article/details/116174774?ops_request_misc=%7B%22request%5Fid%22%3A%22169381126916800225545986%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=169381126916800225545986&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-5-116174774-null-null.142^v93^control&utm_term=python%E6%89%BE%E4%B8%8D%E5%88%B0%E4%B8%8A%E7%BA%A7%E7%9B%AE%E5%BD%95%E7%9A%84%E6%96%87%E4%BB%B6%E5%A4%B9&spm=1018.2226.3001.4187)，为了减少导入包时候的错误，导入同一包(目录)中的模块应使用相对导入(.)，导入其他包中模块应使用绝对导入。
8. 关于python中的解包和打包，*通常用于解包，如`a=[[1,2],[3,4]]`，则使用*a返回两个List，`[1,2]`和`[3,4]`，使用zip进行打包，它会将两个可迭代对象相同位置的元素打包成一个元组，然后返回一个迭代器，例如`a=[1,2]`，`b=[3,4]`，zip(a,b)会得到两个元组`(1,3)，(2,4)`，可以用for来进行迭代。通常在__collate_fn__中，使用zip(*batch)重新组织一批数据，例如get_item函数返回了两个列表a,b，假设batchsize为8，传入到__collate_fn__中的batch是一个包含8个元素的list，其中每个元素都为元组(a,b)，*batch会将list解包，然后zip会将8个元组中的a打包成一个新的元组，然后将8个元组中的b打包成一个新的元组，也就是说得到了两个新元组x(由8个a组成)和y(由8个b组成)，然后分别返回x，y即可。
9. 关于params_group，以及如何为不同层设置不同学习率，参考[1](https://blog.csdn.net/weixin_45464524/article/details/130456843?ops_request_misc=%7B%22request%5Fid%22%3A%22169390449016800192272047%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=169390449016800192272047&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~times_rank-2-130456843-null-null.142^v93^control&utm_term=pytorch%E7%9A%84params_group&spm=1018.2226.3001.4187)，[2](https://zhuanlan.zhihu.com/p/347929433)，[3](https://blog.csdn.net/weixin_45464524/article/details/130477798?ops_request_misc=%7B%22request%5Fid%22%3A%22169407405916800184146887%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=169407405916800184146887&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-130477798-null-null.142^v93^control&utm_term=%E4%BD%BF%E7%94%A8params_group%E5%AF%B9%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E4%B8%8D%E5%90%8C%E5%AD%A6%E4%B9%A0%E7%8E%87&spm=1018.2226.3001.4187)。
10. 关于python中矩阵乘法multiply(). dot().  matmul(). ' * '. '@'的区别，参考:[1](https://blog.csdn.net/u011851421/article/details/83783826?ops_request_misc=&request_id=&biz_id=102&utm_term=@,dot,multiply%E7%9A%84%E5%8C%BA%E5%88%AB&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-83783826.142^v93^control&spm=1018.2226.3001.4187)
11. optimizer.zero_grad和model.zero_grad的区别，参考[1](https://blog.csdn.net/qq_32614873/article/details/128626028?ops_request_misc=%7B%22request%5Fid%22%3A%22169400319916777224491083%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=169400319916777224491083&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-128626028-null-null.142^v93^control&utm_term=model.zero_grad()%E5%92%8Coptimizer.zero_grad()&spm=1018.2226.3001.4187)。
12. pytorch的显存分配机制，参考(https://zhuanlan.zhihu.com/p/424512257)
13. 哪些部分需要to(device)? 损失函数需不需要to(device)? 参考[1](https://blog.csdn.net/virus111222/article/details/129773728?ops_request_misc=&request_id=&biz_id=102&utm_term=pytorch%E5%93%AA%E4%BA%9B%E9%9C%80%E8%A6%81to(device)&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-3-129773728.142^v93^control&spm=1018.2226.3001.4187)
14. 关于分组卷积，即conv2d中groups参数，每个卷积核的通道数为in_channel / groups，然后共有out_channels个卷积核，每个组内有out_channels / groups个卷积核，深度可分离卷积的输入输出通道数以及groups都相同，具体例子参考[1](https://blog.csdn.net/u012633319/article/details/109171775?ops_request_misc=%7B%22request%5Fid%22%3A%22169443420016800180671495%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=169443420016800180671495&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-109171775-null-null.142^v93^control&utm_term=conv2d%E7%9A%84groups%E5%8F%82%E6%95%B0&spm=1018.2226.3001.4187)
15. 关于全局平均池化(GAP)的原理以及实现。参考[1](https://blog.csdn.net/qq_41990294/article/details/128930017?ops_request_misc=%7B%22request%5Fid%22%3A%22169466072616800185863464%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=169466072616800185863464&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-128930017-null-null.142^v94^control&utm_term=globalaveragepooling&spm=1018.2226.3001.4187)

### 第4周:rainbow:
1. 关于模型断点继续训练，参考(https://zhuanlan.zhihu.com/p/611485709),                           (https://zhuanlan.zhihu.com/p/647349640)
2. 关于进度条tqdm的使用，参考(https://blog.csdn.net/wxd1233/article/details/118371404)
3. 关于词嵌入向量torch.nn.Embedding，参考[1](https://blog.csdn.net/athrunsunny/article/details/123068113?spm=1001.2014.3001.5501)
4. 匈牙利算法常用来解决最优指派问题，在DETR里它的作用是为每一个gt匹配到唯一的query，从而使代价矩阵的匹配成功的query和gt的代价和最小，关于匈牙利算法的简单理解，参考[1](https://blog.csdn.net/athrunsunny/article/details/123504015?spm=1001.2014.3001.5502)
5. 各种IOU的介绍，参考[1](https://blog.csdn.net/weixin_43694096/article/details/126455488?ops_request_misc=%7B%22request%5Fid%22%3A%22169544548316800180675276%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=169544548316800180675276&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~times_rank-17-126455488-null-null.142^v94^control&utm_term=ciou,diou,giou&spm=1018.2226.3001.4187)
6. latex配置c和python代码块风格，参考[1](https://blog.csdn.net/Aldielshala/article/details/89840206)，[2](https://zhuanlan.zhihu.com/p/464141424)
7. latex写伪代码，参考(https://zhuanlan.zhihu.com/p/599142563)，(https://zhuanlan.zhihu.com/p/572285214)
8. plt.subplots的使用,参考[1](https://blog.csdn.net/weixin_39258979/article/details/126039763)

### 第5周:rainbow:
1. 使用algorithmic宏包编写伪代码时的分页问题，参考(https://blog.csdn.net/Yiigel/article/details/53840888)
2. cv中的绝对位置编码和相对位置编码，绝对位置编码：一般是可学习的，实现非常简洁，初始化一个形状和经过patch_embedding后的输出x相同的参数，然后直接加到x上，在训练阶段一起训练，有点像加了一个可学习的bias，VIT中使用的就是这种方式。对于相对位置编码：绝对位置编码是在patch_embedding之后，计算自注意力(Q,K,V)之前，而相对位置编码是在计算自注意力的时候加入的，在swin transformer中，将二维位置信息转换为一维位置信息，然后去根据相对位置偏移表查询具体的值，位置偏移表的值是训练出来的，在文章中，相对位置编码效果好于绝对位置编码。具体参考[这里](https://blog.csdn.net/qq_43733107/article/details/127077056)。
3. dense block，每一层将之前所有层的输入进行拼接，之后将输出的特征图传递给之后的所有层，可以可以减轻梯度消失现象，加强特征的融合。参考(https://www.jianshu.com/p/0b8fc900abef)
4. 关于变异系数CV(coefficient of variation)，需要比较两组数据离散程度大小的时候，如果两组数据的测量尺度相差太大，或者数据量纲的不同，直接使用标准差来进行比较不合适，此时就应当消除测量尺度和量纲的影响，而变异系数可以做到这一点，它是原始数据标准差与原始数据平均数的比。CV没有量纲，这样就可以进行客观比较了。事实上，可以认为变异系数和极差. 标准差和方差一样，都是反映数据离散程度的绝对值。其数据大小不仅受变量值离散程度的影响，而且还受变量值平均水平大小的影响。参考[1](https://blog.csdn.net/m0_64799907/article/details/130041485?ops_request_misc=%7B%22request%5Fid%22%3A%22169745656316800225536260%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=169745656316800225536260&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-130041485-null-null.142^v96^control&utm_term=coefficientofvariation&spm=1018.2226.3001.4187)
5. git fetch 和git pull的区别，git fetch是将远程主机的最新内容拉到本地，用户在检查了以后决定是否合并到工作本机分支中。而git pull 则是将远程主机的最新内容拉下来后直接合并，即：git pull = git fetch + git merge，这样可能会产生冲突，需要手动解决。
6. 转置卷积的计算详解，参考[1](https://blog.csdn.net/ZhaoDongyu_AK47/article/details/130602195)

### 第六周:rainbow:
1. 新设备配置git时，验证是否成功ssh -T git@github.com，启智平台同理，参考[1](https://blog.csdn.net/m0_72983118/article/details/130546429)
2. vbox的虚拟环境的复制，只需要复制.vbox, .vdi, vbox-prev文件到相同路径，然后在vbox中注册即可
3. vscode终端支持conda命令，参考[1](https://blog.csdn.net/takedachia/article/details/124694616)
4. list的extend和append的区别：append（）用于在列表末尾添加新的对象，输入参数为对象；extend（）用于在列表末尾追加另一个序列中的多个值，输入对象为元素队列；
5. numpy的ravel和flatten函数的区别：参考[1](https://blog.csdn.net/liuweiyuxiang/article/details/78220080?ops_request_misc=%7B%22request%5Fid%22%3A%22169830186016800186578235%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=169830186016800186578235&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-78220080-null-null.142^v96^pc_search_result_base5&utm_term=ravel%E5%87%BD%E6%95%B0&spm=1018.2226.3001.4187)
6. delctype和auto的区别，二者都用于自动推导类型，但delctype更为灵活，auto所修饰的变量必须被初始化，编译器需要通过初始化来确定auto所代表的类型，即必须要定义变量。若仅希望得到类型，则可以用delctype，如一些排序规则需要使用仿函数，可以用lambda表达式定义比较函数，然后用decltype自动推导。参考(https://zhuanlan.zhihu.com/p/152154499)


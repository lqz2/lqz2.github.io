---
title: 四种位置编码实现
date: 2023-12-02 17:05:53
categories: 资料
math: true
tags:
---
<!-- TOC -->

- [Transformer / VIT / Swin Transformer / MAE中的位置编码实现](#transformer--vit--swin-transformer--mae中的位置编码实现)
    - [Transformer](#transformer)
    - [VIT中的绝对位置编码](#vit中的绝对位置编码)
    - [Swin Transformer的相对位置编码](#swin-transformer的相对位置编码)
    - [MAE](#mae)

<!-- /TOC -->
## Transformer / VIT / Swin Transformer / MAE中的位置编码实现

### Transformer
Transformer中的位置编码是这样定义的：
$$\begin{aligned}PE_{(pos,2i)}&=sin(pos/10000^{2i/d_{\mathrm{model}}})\\PE_{(pos,2i+1)}&=cos(pos/10000^{2i/d_{\mathrm{model}}})\end{aligned}$$

实现：
```
def create_1d_absolute_sincos_pe(n_pos_vec, dim):
    """
    :param n_pos_vec: (tensor)torch.arrge(n_pos,dtype=torch.float32)
    """
    assert dim % 2 == 0, 'dim must be even'
    position_embedding = torch.zeros((n_pos_vec.numel(), dim), dtype=torch.float32)  # [n_pos,dim]
    omega = torch.arange(dim // 2, dtype=torch.float32)  # i
    omega /= dim / 2
    omega = 1 / (1e4**omega)
    out = n_pos_vec.unsqueeze(1) @ omega.unsqueeze(0)  # [n_pos,dim//2]
    position_embedding[:, 0::2] = torch.sin(out)
    position_embedding[:, 1::2] = torch.cos(out)
    return position_embedding
```
### VIT中的绝对位置编码
```
def create_1d_absolute_trainable_pe(n_pos_vec, dim):
    """
    :param n_pos_vec: (tensor)torch.arrge(n_pos,dtype=torch.float32)
    """
    position_embedding = nn.Embedding(n_pos_vec.numel(), dim)
    # initialize the position embedding
    nn.init.constant_(position_embedding.weight, 0)
    return position_embedding
```

### Swin Transformer的相对位置编码
在Swin中，相对位置编码$B$用于计算注意力时：
$$\operatorname{Attention}(Q,K,V)=\operatorname{SoftMax}(QK^T/\sqrt d+B)V$$

```
def get_2d_relative_position_index(height, width):
    m1, m2 = torch.meshgrid(torch.arange(height), torch.arange(width))  # m1每一行相等，m2每一列相等
    coords = torch.stack((m1, m2), dim=0)  # [2,h,w]
    coords_f = torch.flatten(coords, 1)  # [2,h*w]
    relative_coords_bias = coords_f[:, :, None] - coords_f[:, None, :]  # [2,h*w,h*w]
    relative_coords_bias[0, :, :] += height - 1
    relative_coords_bias[1, :, :] += width - 1
    relative_coords_bias[0, :, :] *= relative_coords_bias[1, :, :].max() + 1
    return relative_coords_bias[0, :, :] + relative_coords_bias[1, :, :]


def create_2d_relative_bias_trainable_pe(n_head, height, width, dim):
    position_embedding = nn.Embedding((2 * height - 1) * (2 * width - 1), n_head)  # bias[-h+1,h-1],bias[-w+1,w-1]
    nn.init.constant_(position_embedding.weight, 0)
    relative_position_index = get_2d_relative_position_index(height, width)  # [h*w,h*w],获得相对位置索引
    bias_embedding = position_embedding(torch.flatten(relative_position_index)).reshape(height * width, height * width, n_head)  # [h*w,h*w,n_head]
    bias_embedding = bias_embedding.permute(2, 0, 1).unsqueeze(0)  # [1,n_head,h*w,h*w]
    return position_embedding
```
因为每个patch与其他所有patch之间的位置关系都需要被编码，因此输出后两维是h*w
### MAE
```
def create_2d_absolute_sincos_pe(height, width, dim):
    assert dim % 4 == 0, 'dim must be divisible by 4'
    position_embedding = torch.zeros((height * width, dim), dtype=torch.float32)  # [h*w,dim]
    coords = torch.stack(torch.meshgrid(torch.arange(height, dtype=torch.float32), torch.arange(width, dtype=torch.float32)), dim=0)  # [2,h,w]
    height_embedding = create_1d_absolute_sincos_pe(coords[0].flatten(), dim // 2)  # [h*w,dim//2]
    width_embedding = create_1d_absolute_sincos_pe(coords[1].flatten(), dim // 2)  # [h*w,dim//2]
    position_embedding[:, : dim // 2] = height_embedding  # [h*w,dim//2]
    position_embedding[:, dim // 2 :] = width_embedding  # [h*w,dim//2]
    return position_embedding
```